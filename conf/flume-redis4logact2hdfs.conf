# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = com.supermy.redis.flume.redis.source.RedisSource
a1.sources.r1.channels = c1
#a1.sources.r1.host = 172.20.149.158
a1.sources.r1.host = 132.194.43.153
a1.sources.r1.key = netlogactlist
a1.sources.r1.batch_size = 10000

# 拦截器加入时间

#加入时间戳；文件夹分卷使用
a1.sources.r1.interceptors = i3
#转换数据格式为 lua 脚本
a1.sources.r1.interceptors.i3.type = com.supermy.flume.interceptor.RuleSearchAndReplaceInterceptor$Builder
a1.sources.r1.interceptors.i3.searchReplaceDsl = /home/hadoop/my/apache-flume-1.7.0-bin/conf/g-netlogact-timestamp-search-replace.groovy
#a1.sources.r1.interceptors.i3.searchReplaceDsl = /etc/flume/conf/g-netlogact-timestamp-search-replace.groovy
a1.sources.r1.interceptors.i3.searchReplaceKey = searchReplaceGroovy

# Describe the sink
# a1.sinks.k1.type = logger

# Describe the sink
#a1.sinks.k1.type = logger
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1

#%H%M/%S  每天一个文件夹；每个小时一个文件
a1.sinks.k1.hdfs.path = /user/hadoop/my/%y-%m-%d/
a1.sinks.k1.hdfs.filePrefix = netlogact-
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 1
a1.sinks.k1.hdfs.roundUnit = hour
#测试的时候使用，生产勿用
#a1.sinks.k1.hdfs.useLocalTimeStamp = true


# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 100000
a1.channels.c1.transactionCapacity = 10000

# Bind the source and sink to the channel


#cd /home/hadoop/my
#nohup ../apache-flume-1.7.0-bin/bin/flume-ng agent --conf ../apache-flume-1.7.0-bin/conf --conf-file ../apache-flume-1.7.0-bin/conf/flume-redis4logact2hdfs.conf --name a1 -Dflume.root.logger=INFO,console &
