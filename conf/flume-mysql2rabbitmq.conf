# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
###########sql source#################
# Describe/configure the source
#a1.sources.r1.type = netcat
#a1.sources.r1.bind = 0.0.0.0
#a1.sources.r1.port = 44444

# For each one of the sources, the type is defined
a1.sources.r1.type = org.keedio.flume.source.SQLSource

a1.sources.r1.hibernate.connection.url = jdbc:mysql://mysql:3306/javatest

# Hibernate Database connection properties
a1.sources.r1.hibernate.connection.user = java
a1.sources.r1.hibernate.connection.password = java
a1.sources.r1.hibernate.connection.autocommit = true
a1.sources.r1.hibernate.dialect = org.hibernate.dialect.MySQL5Dialect
a1.sources.r1.hibernate.connection.driver_class = com.mysql.jdbc.Driver

#a1.sources.r1.table = employee1

# Columns to import to kafka (default * import entire row)
#a1.sources.r1.columns.to.select = *

# Query delay, each configured milisecond the query will be sent
a1.sources.r1.run.query.delay=10000

# Status file is used to save last readed row
a1.sources.r1.status.file.path = /var/log/flume
a1.sources.r1.status.file.name = r1.status

# Custom query
#a1.sources.r1.start.from = 19700101000000000000
#a1.sources.r1.custom.query = SELECT * FROM (select DECIMAL(test) * 1000000 AS INCREMENTAL, EMPLOYEE1.* from employee1 UNION select DECIMAL(test) * 1000000 AS INCREMENTAL, EMPLOYEE2.* from employee2) WHERE INCREMENTAL > $@$ ORDER BY INCREMENTAL ASC
a1.sources.r1.start.from = 0
a1.sources.r1.custom.query = select id,foo,bar  from testdata where id > $@$ order by id asc
#a1.sources.r1.custom.query = select id,concat('{foo:',foo,',','bar:',bar,'}')  from testdata where id > $@$ order by id asc


a1.sources.r1.batch.size = 1000
a1.sources.r1.max.rows = 1000

a1.sources.r1.hibernate.connection.provider_class = org.hibernate.connection.C3P0ConnectionProvider
a1.sources.r1.hibernate.c3p0.min_size=1
a1.sources.r1.hibernate.c3p0.max_size=10
###########sql source  csv to json ################
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = search_replace
a1.sources.r1.interceptors.i1.searchPattern = (.+),(.+),(.+)
a1.sources.r1.interceptors.i1.replaceString = { "id" : $1, "foo" : $2, "bar" : $3 }
a1.sources.r1.interceptors.i1.charset = UTF-8
###########interceptors################


# Describe the sink
#a1.sinks.k1.type = logger

a1.sinks.k1.type = com.aweber.flume.sink.rabbitmq.RabbitMQSink
a1.sinks.k1.host = mq
a1.sinks.k1.port = 5672
a1.sinks.k1.virtual-host = statuscheckvhost
a1.sinks.k1.username = heartbeat
a1.sinks.k1.password = alive
a1.sinks.k1.exchange = ex-sync-logs
a1.sinks.k1.routing-key = flume.event
a1.sinks.k1.publisher-confirms = true


# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

#bin/flume-ng agent --conf conf --conf-file conf/flume.conf.example1 --name a1 -Dflume.root.logger=INFO,console
#test
#telnet localhost 44444